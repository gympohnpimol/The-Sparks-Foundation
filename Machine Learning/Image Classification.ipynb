{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras import optimizers\n",
    "\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images dimension\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = 'dataset/train'\n",
    "test_data_dir = 'dataset/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to rescale the pixel values from [0, 255] to [0, 1] interval\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 802 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# automagically retrieve images and their classes for train and validation sets\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "                    train_data_dir,\n",
    "                    target_size=(img_width, img_height), \n",
    "                    batch_size=16, class_mode='binary')\n",
    "\n",
    "test_generator = datagen.flow_from_directory(\n",
    "                    test_data_dir,\n",
    "                    target_size=(img_width, img_height),\n",
    "                    batch_size=32, class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small Conv Net\n",
    "\n",
    "### Model architecture definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(32, 3, 3, input_shape=(img_width, img_height, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(32, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='rmsprop',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epoch = 30\n",
    "nb_train_samples = 2000\n",
    "nb_test_samples = 802"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "125/125 [==============================] - 7s 57ms/step - loss: 0.6615 - accuracy: 0.6000 - val_loss: 0.6473 - val_accuracy: 0.5885\n",
      "Epoch 2/30\n",
      "125/125 [==============================] - 7s 58ms/step - loss: 0.6315 - accuracy: 0.6535 - val_loss: 0.6230 - val_accuracy: 0.6384\n",
      "Epoch 3/30\n",
      "125/125 [==============================] - 8s 64ms/step - loss: 0.6026 - accuracy: 0.6740 - val_loss: 0.5979 - val_accuracy: 0.6820\n",
      "Epoch 4/30\n",
      "125/125 [==============================] - 7s 56ms/step - loss: 0.5784 - accuracy: 0.7005 - val_loss: 0.6223 - val_accuracy: 0.6496\n",
      "Epoch 5/30\n",
      "125/125 [==============================] - 7s 57ms/step - loss: 0.5518 - accuracy: 0.7325 - val_loss: 0.5790 - val_accuracy: 0.7082\n",
      "Epoch 6/30\n",
      "125/125 [==============================] - 7s 56ms/step - loss: 0.5373 - accuracy: 0.7370 - val_loss: 0.5815 - val_accuracy: 0.6895\n",
      "Epoch 7/30\n",
      "125/125 [==============================] - 7s 55ms/step - loss: 0.5050 - accuracy: 0.7515 - val_loss: 0.5778 - val_accuracy: 0.6970\n",
      "Epoch 8/30\n",
      "125/125 [==============================] - 7s 55ms/step - loss: 0.4898 - accuracy: 0.7605 - val_loss: 0.5661 - val_accuracy: 0.7007\n",
      "Epoch 9/30\n",
      "125/125 [==============================] - 7s 55ms/step - loss: 0.4699 - accuracy: 0.7675 - val_loss: 0.5596 - val_accuracy: 0.7232\n",
      "Epoch 10/30\n",
      "125/125 [==============================] - 7s 55ms/step - loss: 0.4398 - accuracy: 0.8095 - val_loss: 0.5368 - val_accuracy: 0.7294\n",
      "Epoch 11/30\n",
      "125/125 [==============================] - 7s 55ms/step - loss: 0.4232 - accuracy: 0.8155 - val_loss: 0.5631 - val_accuracy: 0.7120\n",
      "Epoch 12/30\n",
      "125/125 [==============================] - 7s 55ms/step - loss: 0.3794 - accuracy: 0.8280 - val_loss: 0.5794 - val_accuracy: 0.7107\n",
      "Epoch 13/30\n",
      "125/125 [==============================] - 8s 61ms/step - loss: 0.3638 - accuracy: 0.8330 - val_loss: 0.6160 - val_accuracy: 0.7170\n",
      "Epoch 14/30\n",
      "125/125 [==============================] - 7s 54ms/step - loss: 0.3384 - accuracy: 0.8570 - val_loss: 0.5617 - val_accuracy: 0.7232\n",
      "Epoch 15/30\n",
      "125/125 [==============================] - 7s 53ms/step - loss: 0.3204 - accuracy: 0.8690 - val_loss: 0.6457 - val_accuracy: 0.7045\n",
      "Epoch 16/30\n",
      "125/125 [==============================] - 7s 54ms/step - loss: 0.3090 - accuracy: 0.8635 - val_loss: 0.6363 - val_accuracy: 0.7307\n",
      "Epoch 17/30\n",
      "125/125 [==============================] - 6s 51ms/step - loss: 0.2682 - accuracy: 0.8875 - val_loss: 0.6606 - val_accuracy: 0.7219\n",
      "Epoch 18/30\n",
      "125/125 [==============================] - 7s 52ms/step - loss: 0.2455 - accuracy: 0.8945 - val_loss: 0.6609 - val_accuracy: 0.7282\n",
      "Epoch 19/30\n",
      "125/125 [==============================] - 7s 53ms/step - loss: 0.2243 - accuracy: 0.9105 - val_loss: 0.6893 - val_accuracy: 0.7232\n",
      "Epoch 20/30\n",
      "125/125 [==============================] - 6s 52ms/step - loss: 0.2009 - accuracy: 0.9195 - val_loss: 0.6658 - val_accuracy: 0.7145\n",
      "Epoch 21/30\n",
      "125/125 [==============================] - 7s 53ms/step - loss: 0.1904 - accuracy: 0.9275 - val_loss: 0.7190 - val_accuracy: 0.7369\n",
      "Epoch 22/30\n",
      "125/125 [==============================] - 12s 99ms/step - loss: 0.1637 - accuracy: 0.9435 - val_loss: 0.8172 - val_accuracy: 0.7344\n",
      "Epoch 23/30\n",
      "125/125 [==============================] - 10s 84ms/step - loss: 0.1604 - accuracy: 0.9320 - val_loss: 0.8422 - val_accuracy: 0.7232\n",
      "Epoch 24/30\n",
      "125/125 [==============================] - 7s 54ms/step - loss: 0.1412 - accuracy: 0.9465 - val_loss: 0.8399 - val_accuracy: 0.7382\n",
      "Epoch 25/30\n",
      "125/125 [==============================] - 6s 52ms/step - loss: 0.1205 - accuracy: 0.9510 - val_loss: 0.9867 - val_accuracy: 0.7307\n",
      "Epoch 26/30\n",
      "125/125 [==============================] - 7s 54ms/step - loss: 0.1074 - accuracy: 0.9595 - val_loss: 0.9675 - val_accuracy: 0.7332\n",
      "Epoch 27/30\n",
      "125/125 [==============================] - 8s 60ms/step - loss: 0.1061 - accuracy: 0.9665 - val_loss: 0.9368 - val_accuracy: 0.7469\n",
      "Epoch 28/30\n",
      "125/125 [==============================] - 7s 57ms/step - loss: 0.0942 - accuracy: 0.9700 - val_loss: 0.9927 - val_accuracy: 0.7382\n",
      "Epoch 29/30\n",
      "125/125 [==============================] - 7s 52ms/step - loss: 0.0795 - accuracy: 0.9715 - val_loss: 1.0885 - val_accuracy: 0.7145\n",
      "Epoch 30/30\n",
      "125/125 [==============================] - 7s 52ms/step - loss: 0.0664 - accuracy: 0.9750 - val_loss: 1.1790 - val_accuracy: 0.7219\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7feafa5407f0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, \n",
    "          epochs=nb_epoch, \n",
    "          validation_data=test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 802 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1790086030960083, 0.7219451665878296]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluating on validation set \n",
    "\n",
    "model.evaluate_generator(test_generator, nb_test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation for improving the modelÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen_augmented = ImageDataGenerator(\n",
    "    rescale=1./255,       # normalize pixel values to [0,1]\n",
    "    shear_range=0.2,      # randomly applies shearing transformation\n",
    "    zoom_range=0.2,       # randomly applies shearing transformation\n",
    "    horizontal_flip=True) # randomly flip the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator_augmented = train_datagen_augmented.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=32,\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  63/2000 [..............................] - ETA: 7:42 - loss: 0.5353 - accuracy: 0.7365WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 60000 batches). You may need to use the repeat() function when building your dataset.\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 0.5353 - accuracy: 0.7365 - val_loss: 0.5652 - val_accuracy: 0.7195\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7feade9eafa0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    train_generator_augmented,\n",
    "    steps_per_epoch=2000,\n",
    "    epochs=30,\n",
    "    validation_data=test_generator,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1877: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  warnings.warn('`Model.evaluate_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 802 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.565195620059967, 0.719451367855072]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(test_generator, nb_test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16 + small MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg = Sequential()\n",
    "model_vgg.add(ZeroPadding2D((1, 1), input_shape=(img_width, img_height, 3)))\n",
    "model_vgg.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\n",
    "model_vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\n",
    "model_vgg.add(MaxPooling2D((1, 1), strides=(2, 2), padding='valid'))\n",
    "\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_2'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_3'))\n",
    "model_vgg.add(MaxPooling2D((1, 1), strides=(2, 2)))\n",
    "\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_3'))\n",
    "model_vgg.add(MaxPooling2D((1, 1), strides=(2, 2)))\n",
    "\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_2'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_3'))\n",
    "model_vgg.add(MaxPooling2D((1, 1), strides=(2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the VGG16 model to process samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 802 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator_bottleneck = datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=32,\n",
    "    class_mode=None,\n",
    "    shuffle=False)\n",
    "\n",
    "test_generator_bottleneck = datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=32,\n",
    "    class_mode=None,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array([0] * (nb_train_samples // 2) + [1] * (nb_train_samples // 2))\n",
    "test_labels = np.array([0] * (nb_test_samples // 2) + [1] * (nb_test_samples // 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2000 batches). You may need to use the repeat() function when building your dataset.\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 802 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    }
   ],
   "source": [
    "bottleneck_features_train = model_vgg.predict_generator(train_generator_bottleneck, nb_train_samples)\n",
    "bottleneck_features_validation = model_vgg.predict_generator(test_generator_bottleneck, nb_test_samples)\n",
    "\n",
    "train_labels = np.array([0] * (nb_train_samples // 2) + [1] * (nb_train_samples // 2))\n",
    "test_labels = np.array([0] * (nb_test_samples // 2) + [1] * (nb_test_samples // 2))\n",
    "\n",
    "model_top = Sequential()\n",
    "model_top.add(Flatten())\n",
    "model_top.add(Dense(256, activation='relu'))\n",
    "model_top.add(Dropout(0.5))\n",
    "model_top.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_top.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "125/125 [==============================] - 14s 109ms/step - loss: 0.7018 - accuracy: 0.4885 - val_loss: 0.6930 - val_accuracy: 0.5037\n",
      "Epoch 2/40\n",
      "125/125 [==============================] - 14s 111ms/step - loss: 0.7025 - accuracy: 0.4970 - val_loss: 0.6931 - val_accuracy: 0.4988\n",
      "Epoch 3/40\n",
      "125/125 [==============================] - 14s 111ms/step - loss: 0.7198 - accuracy: 0.4880 - val_loss: 0.6930 - val_accuracy: 0.5037\n",
      "Epoch 4/40\n",
      "125/125 [==============================] - 14s 114ms/step - loss: 0.7024 - accuracy: 0.4985 - val_loss: 0.7012 - val_accuracy: 0.4863\n",
      "Epoch 5/40\n",
      "125/125 [==============================] - 14s 111ms/step - loss: 0.6979 - accuracy: 0.4870 - val_loss: 0.6930 - val_accuracy: 0.5025\n",
      "Epoch 6/40\n",
      "125/125 [==============================] - 14s 112ms/step - loss: 0.7294 - accuracy: 0.4875 - val_loss: 0.6924 - val_accuracy: 0.5025\n",
      "Epoch 7/40\n",
      "125/125 [==============================] - 14s 112ms/step - loss: 0.6955 - accuracy: 0.4945 - val_loss: 0.6903 - val_accuracy: 0.5025\n",
      "Epoch 8/40\n",
      "125/125 [==============================] - 14s 112ms/step - loss: 0.7311 - accuracy: 0.4935 - val_loss: 0.6929 - val_accuracy: 0.5037\n",
      "Epoch 9/40\n",
      "125/125 [==============================] - 14s 112ms/step - loss: 0.7037 - accuracy: 0.5160 - val_loss: 0.6936 - val_accuracy: 0.5087\n",
      "Epoch 10/40\n",
      "125/125 [==============================] - 14s 112ms/step - loss: 0.7214 - accuracy: 0.5120 - val_loss: 0.6928 - val_accuracy: 0.5062\n",
      "Epoch 11/40\n",
      "125/125 [==============================] - 14s 113ms/step - loss: 0.7225 - accuracy: 0.5085 - val_loss: 0.6927 - val_accuracy: 0.4988\n",
      "Epoch 12/40\n",
      "125/125 [==============================] - 14s 113ms/step - loss: 0.7073 - accuracy: 0.5280 - val_loss: 0.6907 - val_accuracy: 0.5150\n",
      "Epoch 13/40\n",
      "125/125 [==============================] - 14s 113ms/step - loss: 0.6998 - accuracy: 0.5315 - val_loss: 0.6902 - val_accuracy: 0.5299\n",
      "Epoch 14/40\n",
      "125/125 [==============================] - 15s 117ms/step - loss: 0.6947 - accuracy: 0.5225 - val_loss: 0.6921 - val_accuracy: 0.5125\n",
      "Epoch 15/40\n",
      "125/125 [==============================] - 15s 116ms/step - loss: 0.7035 - accuracy: 0.5210 - val_loss: 0.6868 - val_accuracy: 0.5262\n",
      "Epoch 16/40\n",
      "125/125 [==============================] - 14s 113ms/step - loss: 0.7221 - accuracy: 0.5205 - val_loss: 0.6909 - val_accuracy: 0.5125\n",
      "Epoch 17/40\n",
      "125/125 [==============================] - 14s 111ms/step - loss: 0.6915 - accuracy: 0.5220 - val_loss: 0.6900 - val_accuracy: 0.5125\n",
      "Epoch 18/40\n",
      "125/125 [==============================] - 14s 113ms/step - loss: 0.6967 - accuracy: 0.5375 - val_loss: 0.6869 - val_accuracy: 0.5387\n",
      "Epoch 19/40\n",
      "125/125 [==============================] - 14s 112ms/step - loss: 0.6816 - accuracy: 0.5415 - val_loss: 0.6882 - val_accuracy: 0.5137\n",
      "Epoch 20/40\n",
      "125/125 [==============================] - 14s 115ms/step - loss: 0.6864 - accuracy: 0.5305 - val_loss: 0.6840 - val_accuracy: 0.5237\n",
      "Epoch 21/40\n",
      "125/125 [==============================] - 14s 114ms/step - loss: 0.6812 - accuracy: 0.5450 - val_loss: 0.6862 - val_accuracy: 0.5224\n",
      "Epoch 22/40\n",
      "125/125 [==============================] - 14s 114ms/step - loss: 0.6828 - accuracy: 0.5335 - val_loss: 0.6785 - val_accuracy: 0.5349\n",
      "Epoch 23/40\n",
      "125/125 [==============================] - 14s 113ms/step - loss: 0.6926 - accuracy: 0.5420 - val_loss: 0.6775 - val_accuracy: 0.5299\n",
      "Epoch 24/40\n",
      "125/125 [==============================] - 14s 115ms/step - loss: 0.6832 - accuracy: 0.5335 - val_loss: 0.6842 - val_accuracy: 0.5312\n",
      "Epoch 25/40\n",
      "125/125 [==============================] - 14s 114ms/step - loss: 0.7036 - accuracy: 0.5495 - val_loss: 0.6831 - val_accuracy: 0.5287\n",
      "Epoch 26/40\n",
      "125/125 [==============================] - 14s 112ms/step - loss: 0.6827 - accuracy: 0.5325 - val_loss: 0.6839 - val_accuracy: 0.5212\n",
      "Epoch 27/40\n",
      "125/125 [==============================] - 14s 113ms/step - loss: 0.6841 - accuracy: 0.5350 - val_loss: 0.6804 - val_accuracy: 0.5337\n",
      "Epoch 28/40\n",
      "125/125 [==============================] - 15s 117ms/step - loss: 0.7110 - accuracy: 0.5420 - val_loss: 0.6847 - val_accuracy: 0.5299\n",
      "Epoch 29/40\n",
      "125/125 [==============================] - 15s 123ms/step - loss: 0.6928 - accuracy: 0.5605 - val_loss: 0.6773 - val_accuracy: 0.5349\n",
      "Epoch 30/40\n",
      "125/125 [==============================] - 14s 113ms/step - loss: 0.6868 - accuracy: 0.5500 - val_loss: 0.6701 - val_accuracy: 0.5524\n",
      "Epoch 31/40\n",
      "125/125 [==============================] - 14s 111ms/step - loss: 0.6793 - accuracy: 0.5530 - val_loss: 0.6744 - val_accuracy: 0.5411\n",
      "Epoch 32/40\n",
      "125/125 [==============================] - 14s 113ms/step - loss: 0.6873 - accuracy: 0.5505 - val_loss: 0.6996 - val_accuracy: 0.5436\n",
      "Epoch 33/40\n",
      "125/125 [==============================] - 14s 115ms/step - loss: 0.6704 - accuracy: 0.5545 - val_loss: 0.6702 - val_accuracy: 0.5399\n",
      "Epoch 34/40\n",
      "125/125 [==============================] - 14s 114ms/step - loss: 0.6791 - accuracy: 0.5510 - val_loss: 0.6732 - val_accuracy: 0.5474\n",
      "Epoch 35/40\n",
      "125/125 [==============================] - 14s 114ms/step - loss: 0.6753 - accuracy: 0.5355 - val_loss: 0.6816 - val_accuracy: 0.5200\n",
      "Epoch 36/40\n",
      "125/125 [==============================] - 14s 110ms/step - loss: 0.6792 - accuracy: 0.5550 - val_loss: 0.7087 - val_accuracy: 0.5387\n",
      "Epoch 37/40\n",
      "125/125 [==============================] - 14s 112ms/step - loss: 0.6766 - accuracy: 0.5390 - val_loss: 0.6839 - val_accuracy: 0.5411\n",
      "Epoch 38/40\n",
      "125/125 [==============================] - 14s 113ms/step - loss: 0.6716 - accuracy: 0.5520 - val_loss: 0.8334 - val_accuracy: 0.5200\n",
      "Epoch 39/40\n",
      "125/125 [==============================] - 14s 113ms/step - loss: 0.6935 - accuracy: 0.5515 - val_loss: 0.6804 - val_accuracy: 0.5436\n",
      "Epoch 40/40\n",
      "125/125 [==============================] - 14s 113ms/step - loss: 0.6656 - accuracy: 0.5555 - val_loss: 0.6700 - val_accuracy: 0.5449\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7feac0ed5d00>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_top.fit(train_generator,\n",
    "          epochs=40, batch_size=32,\n",
    "          validation_data=test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 802 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6699514985084534, 0.5448877811431885]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_top.evaluate_generator(test_generator, nb_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg = Sequential()\n",
    "model_vgg.add(ZeroPadding2D((1, 1), input_shape=(img_width, img_height, 3)))\n",
    "model_vgg.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\n",
    "model_vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\n",
    "model_vgg.add(MaxPooling2D((1, 1), strides=(2, 2), padding='valid'))\n",
    "\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_2'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_3'))\n",
    "model_vgg.add(MaxPooling2D((1, 1), strides=(2, 2)))\n",
    "\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_3'))\n",
    "model_vgg.add(MaxPooling2D((1, 1), strides=(2, 2)))\n",
    "\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_2'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_3'))\n",
    "model_vgg.add(MaxPooling2D((1, 1), strides=(2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=model_vgg.output_shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_vgg.layers[:25]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 802 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "63/63 [==============================] - 20s 314ms/step - loss: 5.0224 - accuracy: 0.0000e+00 - val_loss: 5.0169 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 20s 314ms/step - loss: 5.0122 - accuracy: 0.0000e+00 - val_loss: 5.0075 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 20s 323ms/step - loss: 5.0036 - accuracy: 0.0000e+00 - val_loss: 4.9998 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 21s 327ms/step - loss: 4.9962 - accuracy: 0.0000e+00 - val_loss: 4.9927 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 21s 326ms/step - loss: 4.9897 - accuracy: 0.0000e+00 - val_loss: 4.9867 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 21s 325ms/step - loss: 4.9840 - accuracy: 0.0000e+00 - val_loss: 4.9814 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 21s 325ms/step - loss: 4.9790 - accuracy: 0.0000e+00 - val_loss: 4.9766 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 20s 322ms/step - loss: 4.9744 - accuracy: 0.0000e+00 - val_loss: 4.9723 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 21s 327ms/step - loss: 4.9703 - accuracy: 0.0000e+00 - val_loss: 4.9684 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "56/63 [=========================>....] - ETA: 2s - loss: 4.9668 - accuracy: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "model_vgg.fit(train_generator,\n",
    "          epochs=10,\n",
    "          validation_data=test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 802 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.02806282043457, 0.0]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_vgg.evaluate_generator(test_generator, nb_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 802 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.565195620059967, 0.719451367855072]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(test_generator, nb_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 802 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6699513792991638, 0.5448877811431885]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_top.evaluate_generator(test_generator, nb_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
